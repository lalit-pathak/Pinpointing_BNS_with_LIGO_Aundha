{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pacific-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- import modules --\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "import pandas as pd\n",
    "import h5py \n",
    "import os\n",
    "import sys\n",
    "from itertools import combinations\n",
    "import json\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from analysis_utils import combination_prob_calculator, network_events_calculator, ecdf, new_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monthly-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- reading the evaluated quantities file --\n",
    "O5_path = os.getcwd() + '/../PE_Network_A0_O5/detection_criteria_bns/final_quantities/quantities_of_detected_events_O5_no_outlier.hdf'\n",
    "\n",
    "file = h5py.File(O5_path, 'r')\n",
    "\n",
    "group_list = list(file.keys())\n",
    "param_list = list(file[group_list[0]].keys())\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formal-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- deleting from memory --\n",
    "del(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nutritional-finnish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H1K1A0',\n",
       " 'H1V1A0',\n",
       " 'H1V1K1',\n",
       " 'H1V1K1A0',\n",
       " 'L1H1A0',\n",
       " 'L1H1K1',\n",
       " 'L1H1K1A0',\n",
       " 'L1H1V1',\n",
       " 'L1H1V1A0',\n",
       " 'L1H1V1K1',\n",
       " 'L1H1V1K1A0',\n",
       " 'L1K1A0',\n",
       " 'L1V1A0',\n",
       " 'L1V1K1',\n",
       " 'L1V1K1A0',\n",
       " 'V1K1A0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "annoying-testing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delta_Mc',\n",
       " 'delta_chi_eff',\n",
       " 'delta_dL',\n",
       " 'delta_iota',\n",
       " 'delta_mass_ratio',\n",
       " 'delta_omega',\n",
       " 'delta_pol',\n",
       " 'delta_tc',\n",
       " 'delta_z']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-modification",
   "metadata": {},
   "source": [
    "## $\\Rightarrow$ Defining functions to extract samples from CDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-jewel",
   "metadata": {},
   "source": [
    "$\\mathcal{F(x)} = \\int_{-\\infty}^{x}P(x) dx$\n",
    "\n",
    "$\\Rightarrow \\ Y = \\mathcal{F(x)} $, where  $Y$ ~ Uniform [0, 1)\n",
    "\n",
    "$\\Rightarrow x = \\mathcal{F}^{-1} (Y) $\n",
    "\n",
    "Since $ Y $ is uniformly distributed to get the random sample of Y (i.e. $\\mathcal{F(x)} \\equiv CDF$) , we write the $y$ samples as uniform samples obtained from $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hollow-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_param(sample, rv_dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to evaluate the CDF value (F(X=x)) for a new sample (taken to be our random variable). The function\n",
    "    first evaluates the emperical CDF of the dataset and then using the 'ecdf', it interpolates the value of the \n",
    "    CDF for a given x.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    \n",
    "    sample     : A sample value(x) of the random variable(X) of which the CDF F(X=x) is to be evaluated\n",
    "    \n",
    "    rv_dataset : The dataset of the random variable(X)   [for eg. 'delta_omega' array obtained from fits files]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    cdf_interp_value : Interpolated CDF value for a given sample \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #-- calculating the emperical CDF --\n",
    "    \n",
    "    val, cdf = ecdf(rv_dataset)\n",
    "    \n",
    "    #-- interpolate the CDF at a given 'sample' point --\n",
    "    \n",
    "    cdf_interp_val = np.interp(x=sample, xp=val, fp=cdf)\n",
    "    \n",
    "    return cdf_interp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "superb-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_inv_param(uni_rand_num, rv_dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to evaluate the CDF value (F(X=x)) for a new sample (taken to be our random variable). The function\n",
    "    first evaluates the emperical CDF of the dataset and then using the 'ecdf', it interpolates the value of the \n",
    "    CDF for a given x.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    \n",
    "    uni_rand_num  : A uniform random number U~[0,1)\n",
    "    \n",
    "    rv_dataset : The dataset of the random variable(X)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    rvs_from_CDF = The Random Samples from the CDF using 'Inverse-Sampling Method'\n",
    "    \n",
    "    \"\"\"\n",
    "    if (np.array(uni_rand_num) < 0).any() or (np.array(uni_rand_num) > 1).any():\n",
    "        \n",
    "        raise ValueError('cdfinv requires input in [0,1].')\n",
    "    \n",
    "    #-- boundary of the random variable from the dataset --\n",
    "    \n",
    "    rv_min = min(rv_dataset)\n",
    "    rv_max = max(rv_dataset)\n",
    "    \n",
    "    #-- numpy array of random variable samples, from rv_min to rv_max --\n",
    "    \n",
    "    rv_array = np.linspace(rv_min, rv_max, 10000)\n",
    "    \n",
    "    #-- evaluating the inverse CDF (sampling the random variable from CDF using 'Inverse-Sampling') --\n",
    "    \n",
    "    inv_cdf_interp = interp1d(x=cdf_param(sample=rv_array, rv_dataset=rv_dataset), y=rv_array, kind='cubic', bounds_error=True)\n",
    "    \n",
    "    #-- Note: Here CDF values act as the x-variable and are used to interpolate the random_samples (random variable) --\n",
    "    \n",
    "    #-- Here 'inv_cdf_interp' takes CDF values as I/P and gives the interpolated random_sample as O/P.\n",
    "    \n",
    "    rvs_from_CDF = inv_cdf_interp((cdf_param(sample=rv_max, rv_dataset=rv_dataset)- cdf_param(sample=rv_min, rv_dataset=rv_dataset))*(uni_rand_num) + cdf_param(sample=rv_min, rv_dataset=rv_dataset))\n",
    "    \n",
    "    #-- The above method works on the basis of 'Universality of Uniform Dist' where if CDF of any distribution \n",
    "    #-- is taken as a random variable then it follows a Uniform Distribution U ~ [0,1)\n",
    "\n",
    "    return rvs_from_CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-generation",
   "metadata": {},
   "source": [
    "## $\\Rightarrow$ Back to the main part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-camping",
   "metadata": {},
   "source": [
    "### $\\Rightarrow$ For O5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instructional-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Nested Dictionary --\n",
    "\n",
    "#-- Primary Key = Detector Comb , Secondary Key = delta_Parameter\n",
    "\n",
    "data_O5 = {}\n",
    "\n",
    "f = h5py.File(O5_path, 'r')\n",
    "\n",
    "for group in group_list:\n",
    "    \n",
    "    data_O5[group] = {}\n",
    "    \n",
    "    for param in param_list:\n",
    "        \n",
    "        data_O5[group][param] = np.array(f[group][param])\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-flavor",
   "metadata": {},
   "source": [
    "### $\\Rightarrow$ For O4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "national-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### -- structuring and storing the various parameters data in 'data' from 'quantities_of_interest_main.hdf' --\n",
    "\n",
    "data_O4 = {}\n",
    "\n",
    "O4_path = os.getcwd() + '/../PE_Network_A0_O4/detection_criteria_bns/final_quantities/quantities_of_detected_events_O4_no_outlier.hdf'\n",
    "\n",
    "f = h5py.File(O4_path, 'r')\n",
    "\n",
    "for group in group_list:\n",
    "    \n",
    "    data_O4[group] = {}\n",
    "    \n",
    "    for param in param_list:\n",
    "        \n",
    "        data_O4[group][param] = np.array(f[group][param])\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-sunglasses",
   "metadata": {},
   "source": [
    "## $\\Rightarrow$ Creating a Data file ```quantities_of_interest_with_samples_from_cdf.hdf``` storing ```samples_from_cdf```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-granny",
   "metadata": {},
   "source": [
    "#### The code is first run for ```data_O4``` and then for ```data_O5```. Do not Run the code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tropical-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- setting up the seed --\n",
    "extract_samps_seed = 0\n",
    "\n",
    "np.random.seed(extract_samps_seed)\n",
    "\n",
    "#-- total number of samples to be extracted/ sampled from CDF --\n",
    "tot_samps = 8000\n",
    "\n",
    "#-- main program --\n",
    "\n",
    "for group_name in group_list:\n",
    "    \n",
    "    i = 0       #-- This 'i' is used in stopping the repeated creation of 'group' and hence stop causing \"ValueError: Unable to create group (name already exists)\"\n",
    "    \n",
    "    for param in param_list:\n",
    "        \n",
    "        rv_dataset = data_O4[group_name][param]\n",
    "        \n",
    "        val_temp, cdf_temp = ecdf(rv_dataset)\n",
    "        \n",
    "        unif_rand_num = np.random.rand(tot_samps)\n",
    "        \n",
    "        rvs_from_cdf = cdf_inv_param(uni_rand_num=unif_rand_num, rv_dataset=rv_dataset)\n",
    "        \n",
    "        with h5py.File('O4_quantities_of_detected_events_with_samples_from_cdf_{}.hdf'.format(tot_samps), 'a') as file:    \n",
    "            \n",
    "            if (i==0):\n",
    "                \n",
    "                group = file.create_group(group_name)\n",
    "                \n",
    "                group.create_dataset(param, data = rvs_from_cdf)\n",
    "                i += 1\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                group = file[group_name]\n",
    "\n",
    "                group.create_dataset(param, data = rvs_from_cdf)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "noticed-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- setting up the seed --\n",
    "extract_samps_seed = 0\n",
    "\n",
    "np.random.seed(extract_samps_seed)\n",
    "\n",
    "#-- total number of samples to be extracted/ sampled from CDF --\n",
    "tot_samps = 8000\n",
    "\n",
    "#-- main program --\n",
    "\n",
    "for group_name in group_list:\n",
    "    \n",
    "    i = 0       #-- This 'i' is used in stopping the repeated creation of 'group' and hence stop causing \"ValueError: Unable to create group (name already exists)\"\n",
    "    \n",
    "    for param in param_list:\n",
    "        \n",
    "        rv_dataset = data_O5[group_name][param]\n",
    "        \n",
    "        val_temp, cdf_temp = ecdf(rv_dataset)\n",
    "        \n",
    "        unif_rand_num = np.random.rand(tot_samps)\n",
    "        \n",
    "        rvs_from_cdf = cdf_inv_param(uni_rand_num=unif_rand_num, rv_dataset=rv_dataset)\n",
    "        \n",
    "        with h5py.File('O5_quantities_of_detected_events_with_samples_from_cdf_{}.hdf'.format(tot_samps), 'a') as file:    \n",
    "            \n",
    "            if (i==0):\n",
    "                \n",
    "                group = file.create_group(group_name)\n",
    "                \n",
    "                group.create_dataset(param, data = rvs_from_cdf)\n",
    "                i += 1\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                group = file[group_name]\n",
    "\n",
    "                group.create_dataset(param, data = rvs_from_cdf)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-temple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-concrete",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modular-european",
   "metadata": {},
   "source": [
    "### $\\Rightarrow$ Aside: Repeating the same for the events where LIGO India is subthreshold (SNR < 6) \n",
    "\n",
    "#### (Did not use this although)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "metric-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- loading the data file --\n",
    "\n",
    "inj_data = np.loadtxt(os.getcwd()+'/../PE_Network_A0_O4/detection_criteria_bns/injections_L1H1V1K1A0_O4_SNR_20_to_25.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- all LIGO India (A0) SNRs --\n",
    "\n",
    "snr_a0 = inj_data[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- finding the subthreshold SNR events for LIGO India (A0) --\n",
    "\n",
    "#-- index of the subthreshold SNR events (to be used in futher analysis) --\n",
    "\n",
    "sub_a0_idx = np.where(snr_a0 < 6)[0]\n",
    "\n",
    "print('No. of SubThreshold Events for LIGO India: ',len(sub_a0_idx), '\\n')\n",
    "\n",
    "print('event indices: ',sub_a0_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- setting up the seed --\n",
    "extract_samps_seed_A0_subthr = 0\n",
    "\n",
    "np.random.seed(extract_samps_seed_A0_subthr)\n",
    "\n",
    "#-- total number of samples to be extracted/ sampled from CDF --\n",
    "tot_samps = 3000\n",
    "\n",
    "#-- main program --\n",
    "\n",
    "for group_name in group_list:\n",
    "    \n",
    "    i = 0       #-- This 'i' is used in stopping the repeated creation of 'group' and hence stop causing \"ValueError: Unable to create group (name already exists)\"\n",
    "    \n",
    "    for param in param_list:\n",
    "        \n",
    "        rv_dataset = data[group_name][param][sub_a0_idx]\n",
    "        \n",
    "        val_temp, cdf_temp = ecdf(rv_dataset)\n",
    "        \n",
    "        unif_rand_num = np.random.rand(tot_samps)\n",
    "        \n",
    "        rvs_from_cdf = cdf_inv_param(uni_rand_num=unif_rand_num, rv_dataset=rv_dataset)\n",
    "        \n",
    "        with h5py.File('qoi_A0_subthreshold_samples_from_cdf_{}.hdf'.format(tot_samps), 'a') as file:    \n",
    "            \n",
    "            if (i==0):\n",
    "                \n",
    "                group = file.create_group(group_name)\n",
    "                \n",
    "                group.create_dataset(param, data = rvs_from_cdf)\n",
    "                i += 1\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                group = file[group_name]\n",
    "\n",
    "                group.create_dataset(param, data = rvs_from_cdf)\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycbc_env",
   "language": "python",
   "name": "pycbc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
